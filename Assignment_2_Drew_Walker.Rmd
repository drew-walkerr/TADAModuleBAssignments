---
title: "Assignment_2_Drew_Walker"
author: "Drew Walker"
date: "11/30/2021"
output: html_document
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(caTools)
library(rfUtilities)
library(e1071)
library(caTools)
library(mltest)
library(stringr)
library(randomForest)
library(caTools)
library(caret)
library(caretEnsemble)

```

```{r dataload}
annotated_dataset <- read.csv("TADA_Annotated_data.csv")
unlabeled_dataset <- read.csv("TADA_unlabeled_data.csv")

annotated_dataset <- annotated_dataset %>% 
  rename(CLASS = class)
annotated_text <- annotated_dataset$text


tweet_corpus <- VCorpus(VectorSource(annotated_text))

```

Due to errors in preprocessing, we had to remove emoji characters from the dataset. THis is a potential limitation of the study given the potential for emojis to be used to signify illicit use of substances. (Citation?)

# preprocessing
```{r preprocess}
myStopWords <- c(stopwords())
corpus <- tweet_corpus %>% 
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>% 
  tm_map(removeWords,myStopWords) %>% 
  tm_map(stemDocument)
length(corpus)
```

```{r ngrams-tokenizer}
NLP_tokenizer <- function(x){
  unlist(lapply(ngrams(words(x),1:3),paste,collapse = "_"),use.names=FALSE)
}
n_gram_corpus <- tm_map(corpus,content_transformer(NLP_tokenizer))
```

```{r, viewlength}
length(n_gram_corpus)
n_gram_corpus[[2]]$content
```


## Creating ttraining and test data for ML
```{r creating-training-and-test}

set.seed(1234)
split = sample.split(annotated_dataset$CLASS, SplitRatio = 0.8)
training_ngram_corpus = subset(n_gram_corpus, split==TRUE)

eval_n_gram_corpus = subset(n_gram_corpus,split==FALSE)

training_classes <- subset(annotated_dataset$CLASS,split==TRUE)
eval_classes <- subset(annotated_dataset$CLASS,split==FALSE)
```

# Vectorize
```{r create-dtm}
training_dtm <- DocumentTermMatrix(training_ngram_corpus)
training_dtm_sparse <- removeSparseTerms(training_dtm,0.995)
```

# restricting columns for test set
```{r, restricting}
eval_dtm_sparse <- DocumentTermMatrix(eval_n_gram_corpus,list(dictionary=colnames(training_dtm_sparse)))
```

```{r dataframes}
training_dtm_df <- as.data.frame(as.matrix(training_dtm_sparse))
eval_dtm_df <- as.data.frame(as.matrix(eval_dtm_sparse))
colnames(training_dtm_df) <- make.names(colnames(training_dtm_df))
colnames(eval_dtm_df) <- make.names(colnames(eval_dtm_df))


training_dtm_df$CLASS <- training_classes
training_dtm_df$CLASS <- as.factor(training_dtm_df$CLASS)
```

#SVM Model
```{r svm}
trained_model <- svm(CLASS ~.,data=training_dtm_df)
```


```{r svm-predictions}
svm_predictions <- predict(trained_model, newdata=eval_dtm_df)
```

# SVM Accuracy
```{r accuracy-svm, warning=FALSE}
mltest::ml_test(svm_predictions,eval_classes)
svm_accuracy <- mltest::ml_test(svm_predictions,eval_classes,output.as.table = TRUE)

```

#Random Forest Model
```{r random_forest}
rf_trained_model <- randomForest::randomForest(CLASS ~.,data=training_dtm_df)
```

Random forest predictions
```{r random-forest-predictions}
random_forest_predictions <- predict(rf_trained_model, newdata=eval_dtm_df)
```

# RF Accuracy
```{r accuracy-svm, warning=FALSE}
mltest::ml_test(random_forest_predictions,eval_classes)
rf_accuracy <- mltest::ml_test(random_forest_predictions,eval_classes,output.as.table = TRUE)

save.image("SVM_and_rf_models.Rdata")
```
3rd classifier: logistic regression with LASSO

* https://www.kirenz.com/post/2019-09-16-r-text-mining/#logistic-regression-model


```{r multinomial-log-regression}
library(nnet)
multi_model <- nnet::multinom(CLASS ~., data = training_dtm_df,MaxNWts=100000)
predicted_multi <- multi_model %>% 
  predict(eval_dtm_df)
multi_model_accuracy <- mltest::ml_test(predicted_multi,eval_classes, output.as.table = TRUE)
multi_model_accuracy
```


# Append Predictions
```{r}
length(svm_predictions)
mltest::ml_test()

unlabeled_dataset$CLASS <- best_predictions
```




# Ensemble Classification

With ensemble voting for more categories than binary, may need to add more rules that determine a winner given a tie or instance where each algorithm determines new label

https://github.com/kmutya/Ensemble-Learning-in-R

```{r ensemble}

getmode <- function(v) {
 uniqv <- unique(v)
 uniqv[which.max(tabulate(match(v, uniqv)))]
}
mode_test <- c(1,2,3)
getmode(mode_test)
# Majority vote
# mode(predict) if none, 

```
```{r comparisons}
library(gt)

svm_accuracy <- svm_accuracy %>% 
  mutate(model = "svm") 
rf_accuracy <- rf_accuracy %>% 
  mutate(model = "Random Forest")
rf_accuracy$CLASS <- 0:3

multi_model_accuracy <- multi_model_accuracy %>% 
  mutate(model = "Multiclass Logistic Regression")
multi_model_accuracy$CLASS <- 0:3

combined_model_accuracies <- rbind(svm_accuracy,rf_accuracy,multi_model_accuracy) 

combined_model_accuracies_table <- combined_model_accuracies %>% 
select(model,CLASS,balanced.accuracy,F1,precision,recall) %>% 
  group_by(model) %>% 
  mutate(Label = case_when(CLASS == 0 ~ "Nonmedical Use",
                           CLASS == 1 ~ "Prescribed Use",
                           CLASS == 2 ~ "Information/Mention",
                           CLASS == 3 ~ "Unrelated")) %>% 
           select(model,Label,`Balanced Accuracy`=balanced.accuracy, F1, Precision = precision, Recall = recall)


model_results_gt <- gt(combined_model_accuracies_table)
model_results_gt <- model_results_gt %>% 
  tab_header(
    title = "Model Accuracies",
    subtitle = "Results by Model"
    ) %>% 
  opt_row_striping() %>% 
  tab_options(row_group.background.color = "#35b0ab") %>% 
  opt_table_lines()
model_results_gt

model_results_gt %>% 
  gtsave("Model_results.html", inline_css = TRUE)

```

We opted to use the Multiclass Logistic Regression because of its highest recall and F1 scores predicting Nonmedical use, which is were prioritized due to the importance of detecting every nonmedical opioid use for pharmacosurveillance weighted higher than the risk of over-estimating opioid use. We rationalize this given previous literature's criticisms that social media analyses may underestimate real-world drug use due to the selection bias among Twitter subscribers generalizing to the overall population. 

```{r, expanding-predictions}
unlabeled_dataset_clean <- unlabeled_dataset %>% 
  select(text,city) %>% 
  mutate(id = row_number())
unlabeled_text <- unlabeled_dataset_clean$text
city_corpus <- VCorpus(VectorSource(unlabeled_text))
myStopWords <- c(stopwords())
city_corpus_processed <- city_corpus %>% 
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>% 
  tm_map(removeWords,myStopWords) %>% 
  tm_map(stemDocument)
length(city_corpus)

NLP_tokenizer <- function(x){
  unlist(lapply(ngrams(words(x),1:3),paste,collapse = "_"),use.names=FALSE)
}
city_n_gram_corpus <- tm_map(city_corpus_processed,content_transformer(NLP_tokenizer))

length(city_n_gram_corpus)
city_n_gram_corpus[[2]]$content


city_dtm_sparse <- DocumentTermMatrix(city_n_gram_corpus,list(dictionary=colnames(training_dtm_sparse)))


city_dtm_df <- as.data.frame(as.matrix(city_dtm_sparse))
colnames(city_dtm_df) <- make.names(colnames(city_dtm_df))


predicted_city_data <- multi_model %>% 
  predict(city_dtm_df)

unlabeled_dataset_clean$label <- predicted_city_data

summaries_city <- unlabeled_dataset_clean %>% 
  group_by(city) %>% 
  summarise(nonmedical_use_reports = sum(label == 0)) %>% 
  mutate(population = case_when(city == "A" ~ 500000,
                                city == "B" ~ 10000),
         pop_adjusted_reports = nonmedical_use_reports/population,
         per_100_000 = pop_adjusted_reports*100000)

```

